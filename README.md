# LLM Content Evaluator - KI-gest√ºtzte Inhaltsbewertung

üéØ **Produktive FastAPI-Anwendung** mit **LLM as Judge** Methodologie zur automatisierten Bewertung von Bildungsinhalten mit KI-gest√ºtzter Analyse, deutschen Rechtsstandards und p√§dagogischen Qualit√§tskriterien.

## üöÄ Features

- ‚úÖ **17 Bewertungsschemas** f√ºr umfassende Qualit√§tspr√ºfung
- ‚úÖ **4 Schema-Typen**: Ordinal, Checklist, Binary Gates, Derived
- ‚úÖ **LLM as Judge Methodologie** mit OpenAI GPT-4 Integration
- ‚úÖ **Deutsche Rechtskonformit√§t**: StGB, DSGVO, JuSchG, Pers√∂nlichkeitsrechte
- ‚úÖ **P√§dagogische Qualit√§t**: Didaktik, Neutralit√§t, Sachrichtigkeit, Aktualit√§t
- ‚úÖ **RESTful API** mit vollst√§ndiger OpenAPI/Swagger Dokumentation
- ‚úÖ **Produktionsbereit** mit Error Handling und Validation
- ‚úÖ **Flexible Ausgabe**: Detaillierte Begr√ºndungen und Kriterien-Breakdown

## üìã √úbersicht aller Schemas

### Qualit√§tsbewertung (13 Schemas)

| Schema ID | Typ | Beschreibung | Skala | Kriterien |
|-----------|-----|--------------|-------|-----------|
| `sachrichtigkeit_new` | Checklist | Detaillierte Sachrichtigkeit | 10 Kriterien | Faktentreue, Quellenangaben, Wissenschaftlichkeit |
| `neutralitaet_new` | Checklist | Detaillierte Neutralit√§t | 10 Kriterien | Ausgewogenheit, Objektivit√§t, Meinungsvielfalt |
| `aktualitaet_new` | Checklist | Detaillierte Aktualit√§t | 10 Kriterien | Zeitgem√§√üheit, Relevanz, Updates |
| `didaktik_methodik_new` | Checklist | Detaillierte Didaktik | 10 Kriterien | Lernziele, Methoden, Verst√§ndlichkeit |
| `sprachliche_angemessenheit_new` | Checklist | Detaillierte Sprache | 10 Kriterien | Verst√§ndlichkeit, Stil, Zielgruppe, Grammatik |
| `medial_passend_new` | Checklist | Detaillierte Mediale Passung | 10 Kriterien | Technische Qualit√§t, Interaktivit√§t, Zug√§nglichkeit |
| `sachrichtigkeit_old` | Ordinal | Sachrichtigkeit (0-5) | 6 Stufen | Kompakte Bewertung der Faktentreue |
| `neutralitaet_old` | Ordinal | Neutralit√§t (0-5) | 6 Stufen | Kompakte Bewertung der Ausgewogenheit |
| `aktualitaet_old` | Ordinal | Aktualit√§t (0-5) | 6 Stufen | Kompakte Bewertung der Zeitgem√§√üheit |
| `sprachliche_angemessenheit_old` | Ordinal | Sprache (0-5) | 6 Stufen | Verst√§ndlichkeit, Stil, Zielgruppe |
| `didaktik_methodik_old` | Ordinal | Didaktik (0-5) | 6 Stufen | Kompakte p√§dagogische Bewertung |
| `medial_passend_old` | Ordinal | Mediale Passung (0-5) | 6 Stufen | Medieneignung, Darstellung |
| `overall_quality` | Derived | Gesamtqualit√§t | 0.0-5.0 | Gewichtete Kombination aller Dimensionen |

### Compliance & Rechtssicherheit (4 Schemas)

| Schema ID | Typ | Beschreibung | Rechtsbasis | Pr√ºfbereiche |
|-----------|-----|--------------|-------------|--------------|
| `jugendschutz_gate` | Binary Gate | Jugendschutz | JuSchG, JMStV | Altersgerechte Inhalte, Entwicklungsschutz |
| `strafrecht_gate` | Binary Gate | Strafrecht | StGB, NetzDG | Volksverhetzung, Gewaltverherrlichung |
| `persoenlichkeitsrechte_gate` | Binary Gate | Pers√∂nlichkeitsrechte | DSGVO, KUG | Bildrechte, Datenschutz, Privatsph√§re |
| `rechtliche_compliance` | Derived | Rechtliche Gesamtbewertung | Alle Gates | UND-Verkn√ºpfung aller Compliance-Gates |

## üõ†Ô∏è Installation & Setup

### Voraussetzungen
- Python 3.8+
- OpenAI API Key (f√ºr KI-basierte Bewertung)
- FastAPI, Uvicorn, Pydantic (automatisch installiert)

### Schnellstart

```bash
# Repository klonen
git clone <repository-url>
cd api-scoring-quality/api-eval-25

# Umgebung einrichten
cp .env.example .env
# .env mit OpenAI API Key bearbeiten:
# OPENAI_API_KEY=your_api_key_here

# Dependencies installieren
pip install fastapi uvicorn openai pydantic loguru pyyaml

# Server starten
python main.py
```

üåê **Server l√§uft auf: http://localhost:8001**  
üìö **API Dokumentation: http://localhost:8001/docs**  
üîç **Alternative Docs: http://localhost:8001/redoc**

## üîó API Endpoints

### üè• Health Check
```http
GET /health
```
**Status:** ‚úÖ Funktionsf√§hig  
**Zweck:** API-Status und geladene Schemas pr√ºfen

### üìã Schema-√úbersicht  
```http
GET /schemes
```
**Status:** ‚úÖ Funktionsf√§hig  
**Zweck:** Alle 15 verf√ºgbaren Bewertungsschemas auflisten

### üéØ Text-Bewertung
```http
POST /evaluate
```
**Status:** ‚úÖ Funktionsf√§hig  
**Zweck:** KI-gest√ºtzte Bewertung von Bildungsinhalten

**Beispiel-Request:**
```json
{
    "text": "Die deutsche Wiedervereinigung war ein komplexer politischer Prozess...",
    "schemes": ["neutralitaet_new", "sachrichtigkeit_old", "rechtliche_compliance"],
    "include_reasoning": true
}
```

**Beispiel-Response:**
```json
{
    "results": [
        {
            "scheme_id": "neutralitaet_new",
            "value": 4.2,
            "label": "Weitgehend neutral",
            "confidence": 0.88,
            "reasoning": "Der Text stellt verschiedene Perspektiven ausgewogen dar...",
            "criteria": {
                "ausgewogenheit": {"value": 4, "reasoning": "Mehrere Standpunkte ber√ºcksichtigt"},
                "objektivitaet": {"value": 4, "reasoning": "Sachliche Darstellung ohne Wertungen"}
            }
        }
    ],
    "gates_passed": true,
    "metadata": {
        "processing_time_ms": 1250,
        "model_used": "gpt-4"
    }
}
```

## üìñ YAML Schema-Parameter - Detaillierte Anleitung

### Grundstruktur aller Schemas

Jedes YAML-Schema folgt einer einheitlichen Grundstruktur:

```yaml
# Eindeutige Identifikation
id: schema_identifier
name: "Benutzerfreundlicher Anzeigename"
description: "Detaillierte Beschreibung des Bewertungszwecks"
dimension: dimension_name  # Technischer Dimensionsname
type: ordinal|checklist|binary_gate|derived
version: "1.0"

# Ausgabeformat definieren
output_range:
  min: 0      # Minimaler Wert
  max: 5      # Maximaler Wert  
  type: int   # int, float oder boolean
  values: [0, 1, 2, 3, 4, 5]  # Optional: Erlaubte Werte
```

**Pflichtfelder:**
- `id`: Eindeutige Schema-ID (keine Leerzeichen, Kleinbuchstaben)
- `name`: Anzeigename f√ºr Benutzeroberfl√§che
- `type`: Schema-Typ bestimmt Bewertungslogik
- `output_range`: Definiert m√∂gliche Ausgabewerte

### Schema-Typen im Detail

#### 1. Ordinal Schema - Anker-basierte Bewertung (0-5 Skala)

Ordinal-Schemas verwenden vordefinierte Anker f√ºr konsistente Bewertungen:

```yaml
id: neutralitaet_old
name: "Neutralit√§t (Ordinal)"
description: "Bewertung der politischen und weltanschaulichen Neutralit√§t"
dimension: neutrality
type: ordinal
version: "1.0"

output_range:
  min: 0
  max: 5
  type: int

# Bewertungsstrategie
strategy: first_match  # oder best_fit

# Anker in absteigender Reihenfolge (5 ‚Üí 0)
anchors:
  - value: 5
    label: "Vollst√§ndig neutral"
    criteria: |
      - Alle Standpunkte werden fair dargestellt
      - Keine erkennbare politische Tendenz
      - Ausgewogene Quellenauswahl
      - Objektive Sprache durchgehend
      
  - value: 4
    label: "Weitgehend neutral"
    criteria: |
      - √úberwiegend ausgewogene Darstellung
      - Minimale Tendenz erkennbar
      - Verschiedene Perspektiven ber√ºcksichtigt
      
  - value: 3
    label: "Teilweise neutral"
    criteria: |
      - Grunds√§tzlich um Neutralit√§t bem√ºht
      - Deutliche Tendenz in eine Richtung
      - Nicht alle Standpunkte gleichwertig dargestellt
      
  - value: 2
    label: "Wenig neutral"
    criteria: |
      - Starke Tendenz erkennbar
      - Einseitige Quellenauswahl
      - Gegenpositionen nur oberfl√§chlich behandelt
      
  - value: 1
    label: "Einseitig"
    criteria: |
      - Deutlich parteiische Darstellung
      - Wichtige Gegenpositionen fehlen
      - Wertende Sprache dominiert
      
  - value: 0
    label: "Stark einseitig"
    criteria: |
      - Propagandistische Darstellung
      - Keine Ber√ºcksichtigung anderer Standpunkte
      - Manipulative oder hetzerische Sprache

# Fallback bei unklaren F√§llen
default:
  value: 0
  label: "Unbewertet"
  reasoning: "Keine ausreichenden Informationen f√ºr Bewertung"
  confidence: 0.0

# Zus√§tzliche Metadaten
labels:
  "5": "Exzellent neutral"
  "4": "Gut neutral"
  "3": "Akzeptabel"
  "2": "Verbesserungsbed√ºrftig"
  "1": "Problematisch"
  "0": "Inakzeptabel"
```

**Strategien:**
- `first_match`: Erste passende Bewertung wird verwendet
- `best_fit`: Beste √úbereinstimmung wird gesucht

**Vollst√§ndiges Beispiel:** `neutralitaet_old.yaml`

#### 2. Checklist Schema - Kriterien-basierte Bewertung

Checklist-Schemas bewerten anhand gewichteter Einzelkriterien mit **kompakter Strukturbeschreibung**:

```yaml
id: sachrichtigkeit_new
name: "Sachrichtigkeit (Detailliert)"
description: "Umfassende Bewertung der faktischen Korrektheit"
dimension: factuality
type: checklist_additive
version: "1.0"

output_range:
  min: 0.0
  max: 5.0
  type: float

# Einzelkriterien mit strukturierten Werten (NEUE KOMPAKTE STRUKTUR)
items:
  - id: faktentreue
    prompt: "Sind alle Fakten und Aussagen korrekt und verifizierbar?"
    weight: 2.5
    values:
      1: {score: 0.25, description: "Viele falsche oder ungepr√ºfte Aussagen"}
      2: {score: 0.5, description: "Einige faktische Fehler oder Ungenauigkeiten"}
      3: {score: 0.75, description: "√úberwiegend korrekte Fakten, wenige Fehler"}
      4: {score: 1.0, description: "Alle Fakten korrekt und gut verifizierbar"}
      na: null
      
  - id: quellenangaben
    prompt: "Sind alle Behauptungen mit seri√∂sen Quellen belegt?"
    weight: 2.0
    values:
      1: {score: 0.25, description: "Keine oder unzuverl√§ssige Quellenangaben"}
      2: {score: 0.5, description: "Wenige oder teilweise fragw√ºrdige Quellen"}
      3: {score: 0.75, description: "Gute Quellenangaben, meist seri√∂s"}
      4: {score: 1.0, description: "Vollst√§ndige, seri√∂se und aktuelle Quellenangaben"}
      na: null

# Aggregation mit gewichtetem Mittelwert
aggregator:
  strategy: weighted_mean
  params:
    missing: ignore
    scale_factor: 5.0  # Skaliert von 0-1 auf 0-5

# Labels f√ºr Ausgabe
labels:
  "0.0": "Unzureichend"
  "1.0": "Mangelhaft"
  "2.0": "Ausreichend"
  "3.0": "Befriedigend"
  "4.0": "Gut"
  "5.0": "Sehr gut"
```

**Neue Kompakte Struktur:**
- **Strukturierte Werte:** `{score: 0.25, description: "..."}` statt einfacher Kommentare
- **Bessere LLM-Prompts:** Beschreibungen werden direkt in Bewertungsprompts eingebunden
- **Einheitliche Skalierung:** Alle Kriterien verwenden 1-4 Level mit 0.25-1.0 Scores

**Vollst√§ndiges Beispiel:** `sachrichtigkeit_new.yaml`, `neutralitaet_new.yaml`, `sprachliche_angemessenheit_new.yaml`, `medial_passend_new.yaml`

#### 3. Binary Gate - Pass/Fail Bewertung

Binary Gates sind K.O.-Kriterien f√ºr Compliance-Pr√ºfungen:

```yaml
id: strafrecht_gate
name: "Strafrechtliche Unbedenklichkeit"
description: "Pr√ºfung auf Verst√∂√üe gegen deutsches Strafrecht"
dimension: legal_compliance
type: binary_gate
version: "1.0"

output_range:
  values: [true, false]
  type: boolean

# Pr√ºfkriterien (wird in criteria-Feld dokumentiert)
criteria: |
  Automatische Pr√ºfung auf strafrechtlich relevante Inhalte:
  - Volksverhetzung (¬ß 130 StGB)
  - Gewaltverherrlichung (¬ß 131 StGB)
  - Beleidigung und Verleumdung (¬ß 185-187 StGB)
  - Bedrohung (¬ß 241 StGB)
  - Verfassungswidrige Symbole (¬ß 86a StGB)

# Gate-Regeln (Reihenfolge wichtig!)
gate_rules:
  - condition: "volksverhetzung"
    action: reject
    reason: "Inhalt enth√§lt volksverhetzende √Ñu√üerungen (¬ß 130 StGB)"
    severity: critical
    legal_reference: "¬ß 130 StGB"
    confidence: 0.9
    
  - condition: "gewaltverherrlichung"
    action: reject
    reason: "Inhalt verherrlicht Gewalt (¬ß 131 StGB)"
    severity: critical
    legal_reference: "¬ß 131 StGB"
    confidence: 0.85
    
  - condition: "beleidigung"
    action: reject
    reason: "Inhalt enth√§lt Beleidigungen (¬ß 185-187 StGB)"
    severity: medium
    legal_reference: "¬ß 185-187 StGB"
    confidence: 0.7
    
  - condition: "bedrohung"
    action: reject
    reason: "Inhalt enth√§lt Bedrohungen (¬ß 241 StGB)"
    severity: high
    legal_reference: "¬ß 241 StGB"
    confidence: 0.8

# Standard-Aktion wenn keine Regel greift
default_action: pass

# Automatische Keyword-Erkennung
detection_patterns:
  volksverhetzung:
    - "holocaustleugnung"
    - "rassenhass"
    - "judenhass"
    - "ausl√§nder raus"
    - "vernichtung"
    
  gewaltverherrlichung:
    - "folter verherrlichen"
    - "gewalt glorifizieren"
    - "hinrichtung feiern"
    - "sadismus"
    
  beleidigung:
    - "idiot"
    - "schwachkopf"
    - "versager"
    # (Kontext-abh√§ngig)
    
  bedrohung:
    - "ich bringe dich um"
    - "du bist tot"
    - "warte nur ab"

# Severity-Level Definitionen
severity_levels:
  critical: "Schwere Straftaten mit Gef√§ngnisstrafe"
  high: "Straftaten mit erheblicher gesellschaftlicher Relevanz"
  medium: "Rechtsverst√∂√üe mit zivilrechtlichen Konsequenzen"
  low: "Geringf√ºgige Rechtsverst√∂√üe"

# Compliance-Referenzen
compliance_references:
  - "Strafgesetzbuch (StGB)"
  - "Netzwerkdurchsetzungsgesetz (NetzDG)"
  - "Digital Services Act (DSA)"
```

**Gate-Logik:**
1. Regeln werden in Reihenfolge gepr√ºft
2. Erste zutreffende Regel bestimmt Ergebnis
3. Bei keiner Regel: `default_action`

**Severity-Levels:**
- `critical`: Sofortige Sperrung erforderlich
- `high`: Manuelle Pr√ºfung erforderlich
- `medium`: Warnung an Nutzer
- `low`: Hinweis ausreichend

**Vollst√§ndiges Beispiel:** `strafrecht_gate.yaml`

#### 4. Derived Schema - Kombination anderer Schemas

Derived Schemas kombinieren Ergebnisse anderer Schemas:

```yaml
id: overall_quality
name: "Gesamtqualit√§t"
description: "Gewichtete Kombination aller Qualit√§tsdimensionen"
dimension: overall_quality
type: derived
version: "2.0"

output_range:
  min: 0.0
  max: 5.0
  type: float

# Abh√§ngigkeiten (m√ºssen vorher ausgewertet werden)
dependencies:
  - neutralitaet_old
  - aktualitaet_old
  - sachrichtigkeit_old
  - sprachliche_angemessenheit_old
  - didaktik_methodik_old
  - medial_passend_old

# Kombinationsregeln
rules:
  - conditions:
      - dimension: neutrality
        operator: ">="
        value: 0
      - dimension: factuality
        operator: ">="
        value: 0
    value: "weighted_average"
    label: "Gewichtete Gesamtbewertung"
    reasoning: "Berechnung basierend auf gewichteten Einzeldimensionen"
    confidence: 0.9
    
    # Gewichtung der Dimensionen
    weights:
      neutrality: 2.0              # Neutralit√§t (wichtig)
      timeliness: 1.5              # Aktualit√§t
      factuality: 2.5              # Sachrichtigkeit (sehr wichtig)
      language_appropriateness: 1.5 # Sprachliche Angemessenheit
      pedagogy: 2.0                # Didaktik/Methodik (wichtig)
      media_appropriateness: 1.0    # Mediale Passung
      
  # Spezialregel f√ºr niedrige Sachrichtigkeit
  - conditions:
      - dimension: factuality
        operator: "<"
        value: 2.0
    value: 1.0
    label: "Unzureichende Sachrichtigkeit"
    reasoning: "Niedrige Sachrichtigkeit f√ºhrt zu schlechter Gesamtbewertung"
    confidence: 0.95

# Fallback
default:
  value: 0.0
  label: "Unbewertet"
  reasoning: "Keine ausreichenden Daten f√ºr Gesamtbewertung"
  confidence: 0.0

# Ausgabe-Labels
labels:
  "4.5-5.0": "Exzellente Qualit√§t"
  "3.5-4.4": "Gute Qualit√§t"
  "2.5-3.4": "Akzeptable Qualit√§t"
  "1.5-2.4": "Verbesserungsbed√ºrftig"
  "0.0-1.4": "Unzureichende Qualit√§t"

# Mathematische Operationen
calculation_method: weighted_average
normalization: true
```

**Kombinationslogik:**
- `weighted_average`: Gewichteter Durchschnitt
- `min`: Minimum aller Werte
- `max`: Maximum aller Werte
- `and_gate`: Alle m√ºssen TRUE sein
- `or_gate`: Mindestens einer TRUE

**Operatoren f√ºr Bedingungen:**
- `==`: Gleich
- `!=`: Ungleich
- `>`: Gr√∂√üer
- `>=`: Gr√∂√üer oder gleich
- `<`: Kleiner
- `<=`: Kleiner oder gleich
- `in`: Wert in Liste
- `not_in`: Wert nicht in Liste

**Vollst√§ndiges Beispiel:** `overall_quality.yaml`

### Erweiterte YAML-Features

#### Conditional Logic (Bedingte Logik)

```yaml
rules:
  # Mehrere Bedingungen (UND-verkn√ºpft)
  - conditions:
      - dimension: neutrality
        operator: ">="
        value: 3
      - dimension: factuality
        operator: ">="
        value: 4
    value: 4.5
    label: "Hohe Qualit√§t"
    
  # ODER-Verkn√ºpfung mit condition_logic
  - condition_logic: "OR"
    conditions:
      - dimension: neutrality
        operator: "=="
        value: 5
      - dimension: factuality
        operator: "=="
        value: 5
    value: 4.0
    label: "Exzellenz in einer Dimension"
```

#### Gate Logic f√ºr Binary Schemas

```yaml
# UND-Verkn√ºpfung (alle Gates m√ºssen TRUE sein)
gate_logic: "AND"

# ODER-Verkn√ºpfung (mindestens ein Gate TRUE)
gate_logic: "OR"

# Beispiel: Rechtliche Unbedenklichkeit
dependencies:
  - content_gate
  - jugendschutz_gate
  - strafrecht_gate
  - persoenlichkeitsrechte_gate

rules:
  - conditions:
      - dimension: content_safety
        operator: "=="
        value: true
      - dimension: youth_protection
        operator: "=="
        value: true
      - dimension: legal_compliance
        operator: "=="
        value: true
      - dimension: personality_rights
        operator: "=="
        value: true
    value: true
    label: "Rechtlich unbedenklich"
```

#### Metadata und Dokumentation

```yaml
# Zus√§tzliche Metadaten
metadata:
  author: "Qualit√§tsteam"
  created: "2025-01-17"
  last_modified: "2025-01-17"
  review_cycle: "quarterly"
  
# Dokumentation f√ºr Entwickler
documentation:
  purpose: "Bewertung der Neutralit√§t in Bildungsinhalten"
  methodology: "Anker-basierte Bewertung mit KI-Unterst√ºtzung"
  validation: "Getestet mit 100+ Beispieltexten"
  
# Konfiguration f√ºr KI-Bewertung
ai_config:
  model: "gpt-4"
  temperature: 0.1
  max_tokens: 500
  prompt_template: "Bewerte die Neutralit√§t des folgenden Textes..."
```

### Validierung und Testing

#### Schema-Validierung

```bash
# YAML-Syntax pr√ºfen
python -c "import yaml; yaml.safe_load(open('schemes/schema.yaml'))"

# Schema-Struktur validieren
python -m core.evaluation --validate-schema schemes/schema.yaml

# Alle Schemas validieren
python -m core.evaluation --validate-all
```

#### Test-Cases definieren

```yaml
# Optional: Test-Cases im Schema
test_cases:
  - input: "Neutral formulierter Text √ºber Politik..."
    expected_value: 4
    expected_label: "Weitgehend neutral"
    
  - input: "Stark einseitiger Text..."
    expected_value: 1
    expected_label: "Einseitig"
```

### Best Practices f√ºr YAML-Schemas

#### 1. Naming Conventions
```yaml
# Schema-IDs: lowercase, underscore
id: sachrichtigkeit_new

# Dimensionen: englisch, lowercase
dimension: factuality

# Kriterien-IDs: deutsch, underscore
criteria:
  - id: faktentreue
  - id: quellenangaben
```

#### 2. Gewichtung
```yaml
# Wichtigste Kriterien: 2.0-2.5
# Normale Kriterien: 1.0-1.5
# Erg√§nzende Kriterien: 0.5-1.0
weights:
  factuality: 2.5      # Sehr wichtig
  neutrality: 2.0      # Wichtig
  timeliness: 1.5      # Normal
  media_fit: 1.0       # Erg√§nzend
```

#### 3. Confidence-Werte
```yaml
# Hohe Sicherheit: 0.9-1.0
# Mittlere Sicherheit: 0.7-0.8
# Niedrige Sicherheit: 0.5-0.6
confidence: 0.9
```

#### 4. Dokumentation
```yaml
# Immer aussagekr√§ftige Beschreibungen
description: "Bewertung der faktischen Korrektheit und Quellenqualit√§t"

# Konkrete Beispiele f√ºr Kriterien
examples:
  - "Zahlen sind durch seri√∂se Quellen belegt"
  - "Historische Fakten sind korrekt dargestellt"
```

## üîÑ Derived Schemas - Kombinationslogik

### UND-Verkn√ºpfung (AND Logic)
```yaml
gate_logic: "AND"
```
Alle Bedingungen m√ºssen erf√ºllt sein.

**Beispiel:** `rechtliche_unbedenklichkeit_derived.yaml`
- Alle 4 Gates m√ºssen `true` sein
- Ein `false` f√ºhrt zur Gesamtablehnung

### ODER-Verkn√ºpfung (OR Logic)
```yaml
gate_logic: "OR"
```
Mindestens eine Bedingung muss erf√ºllt sein.

### Gewichtete Kombination
```yaml
rules:
  - conditions: [...]
    value: "weighted_average"
    weights:
      neutrality: 2.0
      factuality: 2.5
      timeliness: 1.5
```

## üß™ Live-Tests & Swagger UI

**Interaktive API-Tests:** http://localhost:8001/docs

- ‚úÖ **"Try it out" Funktionalit√§t** f√ºr alle Endpoints
- ‚úÖ **Vollst√§ndige Dokumentation** mit Beispielen
- ‚úÖ **Schema-Validierung** und Error Handling
- ‚úÖ **Echtzeit-Tests** direkt im Browser

## üìä Praktische Beispiele

### Beispiel 1: Vollst√§ndige Qualit√§tsbewertung

**Anwendungsfall:** Bewertung von Bildungsinhalten vor Ver√∂ffentlichung

```python
import httpx

# Request
response = httpx.post("http://localhost:8001/evaluate", json={
    "text": "Die deutsche Wiedervereinigung war ein komplexer politischer Prozess, der 1989 mit dem Fall der Berliner Mauer begann. Verschiedene politische Akteure trugen zu diesem historischen Ereignis bei, darunter B√ºrgerbewegungen in der DDR, die Politik der Sowjetunion unter Gorbatschow und die diplomatischen Bem√ºhungen der Bundesregierung.",
    "schemes": [
        "overall_quality",
        "rechtliche_compliance"
    ],
    "include_reasoning": true
})

# Response
result = response.json()
print(f"Gesamtqualit√§t: {result['results'][0]['value']}/5.0")
print(f"Rechtssicherheit: {'‚úì' if result['results'][1]['value'] == 1 else '‚úó'}")
```

**Erwartete Ausgabe:**
```json
{
    "results": [
        {
            "scheme_id": "overall_quality",
            "value": 4.2,
            "label": "Gute Qualit√§t",
            "confidence": 0.88,
            "reasoning": "Der Text zeigt hohe Sachrichtigkeit und Neutralit√§t...",
            "criteria": {
                "neutralitaet_old": {"value": 4, "label": "Weitgehend neutral"},
                "sachrichtigkeit_old": {"value": 5, "label": "Vollst√§ndig sachrichtig"},
                "aktualitaet_old": {"value": 4, "label": "Gut aktuell"}
            }
        },
        {
            "scheme_id": "rechtliche_compliance",
            "value": 1,
            "label": "PASS",
            "confidence": 0.95,
            "reasoning": "Alle rechtlichen Pr√ºfungen bestanden"
        }
    ]
}
```

### Beispiel 2: Detaillierte Qualit√§tspr√ºfung

**Anwendungsfall:** Wissenschaftliche Artikel mit Fokus auf Faktentreue

```python
# Request f√ºr detaillierte Bewertung
response = httpx.post("http://localhost:8001/evaluate", json={
    "text": "Laut einer Studie der Universit√§t M√ºnchen aus dem Jahr 2023 zeigen 78% der befragten Sch√ºler verbesserte Lernleistungen bei Verwendung digitaler Medien. Die Studie basiert auf einer Stichprobe von 1.200 Sch√ºlern aus 15 bayerischen Gymnasien.",
    "schemes": [
        "sachrichtigkeit_new",
        "neutralitaet_new",
        "didaktik_methodik_new"
    ],
    "include_reasoning": true
})

# Detaillierte Kriterien-Analyse
for result in response.json()['results']:
    print(f"\n{result['scheme_id']}: {result['value']}/5.0")
    for criterion, details in result['criteria'].items():
        print(f"  - {criterion}: {details['value']} - {details['reasoning'][:50]}...")
```

### Beispiel 3: Compliance-Schnellpr√ºfung

**Anwendungsfall:** User-Generated Content vor Freischaltung

```python
# Rechtliche Schnellpr√ºfung
response = httpx.post("http://localhost:8001/evaluate", json={
    "text": "Das ist ein normaler Kommentar zu einem Bildungsthema ohne problematische Inhalte.",
    "schemes": [
        "jugendschutz_gate",
        "strafrecht_gate",
        "persoenlichkeitsrechte_gate"
    ],
    "include_reasoning": false  # Schnelle Pr√ºfung ohne Details
})

# Einfache Pass/Fail Auswertung
all_passed = all(r['value'] == 1 for r in response.json()['results'])
print(f"Content freigegeben: {'‚úì' if all_passed else '‚úó'}")

# Detaillierte Fehleranalyse bei Problemen
if not all_passed:
    failed_gates = [r['scheme_id'] for r in response.json()['results'] if r['value'] == 0]
    print(f"Problematische Bereiche: {', '.join(failed_gates)}")
```

### Beispiel 4: Batch-Verarbeitung

**Anwendungsfall:** Bewertung mehrerer Inhalte

```python
import asyncio
import httpx

async def evaluate_content(client, text, schemes):
    """Einzelne Inhaltsbewertung"""
    response = await client.post("/evaluate", json={
        "text": text,
        "schemes": schemes,
        "include_reasoning": false
    })
    return response.json()

async def batch_evaluate():
    """Batch-Bewertung mehrerer Inhalte"""
    contents = [
        "Lerninhalt 1: Geschichte der Demokratie...",
        "Lerninhalt 2: Mathematische Grundlagen...",
        "Lerninhalt 3: Naturwissenschaftliche Experimente..."
    ]
    
    async with httpx.AsyncClient(base_url="http://localhost:8001") as client:
        tasks = [
            evaluate_content(client, content, ["overall_quality", "rechtliche_compliance"])
            for content in contents
        ]
        results = await asyncio.gather(*tasks)
    
    # Ergebnisse auswerten
    for i, result in enumerate(results):
        quality = result['results'][0]['value']
        compliance = result['results'][1]['value']
        print(f"Inhalt {i+1}: Qualit√§t {quality}/5.0, Compliance {'‚úì' if compliance else '‚úó'}")

# Ausf√ºhrung
asyncio.run(batch_evaluate())
```

### Beispiel 5: Error Handling

**Anwendungsfall:** Robuste Fehlerbehandlung

```python
import httpx
from typing import Dict, Any

def safe_evaluate(text: str, schemes: list) -> Dict[str, Any]:
    """Sichere Bewertung mit Fehlerbehandlung"""
    try:
        response = httpx.post(
            "http://localhost:8001/evaluate",
            json={
                "text": text,
                "schemes": schemes,
                "include_reasoning": true
            },
            timeout=30.0  # 30 Sekunden Timeout
        )
        response.raise_for_status()
        return {
            "success": True,
            "data": response.json()
        }
    
    except httpx.TimeoutException:
        return {
            "success": False,
            "error": "Timeout: Bewertung dauerte zu lange",
            "retry_recommended": True
        }
    
    except httpx.HTTPStatusError as e:
        return {
            "success": False,
            "error": f"HTTP {e.response.status_code}: {e.response.text}",
            "retry_recommended": e.response.status_code >= 500
        }
    
    except Exception as e:
        return {
            "success": False,
            "error": f"Unerwarteter Fehler: {str(e)}",
            "retry_recommended": False
        }

# Verwendung
result = safe_evaluate(
    "Beispieltext f√ºr Bewertung",
    ["neutralitaet_new", "sachrichtigkeit_new"]
)

if result["success"]:
    print("Bewertung erfolgreich:", result["data"])
else:
    print("Fehler:", result["error"])
    if result["retry_recommended"]:
        print("Wiederholung empfohlen")
```

## ‚öñÔ∏è Deutsche Rechtskonformit√§t

### Abgedeckte Gesetze
- ‚úÖ **Strafgesetzbuch (StGB)**: Volksverhetzung, Gewaltverherrlichung, Beleidigung
- ‚úÖ **Jugendschutzgesetz (JuSchG)**: Altersgerechte Inhalte, Entwicklungsschutz
- ‚úÖ **DSGVO**: Datenschutz, Pers√∂nlichkeitsrechte
- ‚úÖ **Kunsturhebergesetz (KUG)**: Bildrechte, Recht am eigenen Bild
- ‚úÖ **NetzDG**: Strafbare Inhalte in sozialen Netzwerken
- ‚úÖ **Digital Services Act (DSA)**: EU-Digitalrecht

### Automatische Risikobewertung
- üî¥ **Critical**: Schwere Straftaten ‚Üí Sofortige Sperrung
- üü† **High**: Erhebliche Rechtsverst√∂√üe ‚Üí Manuelle Pr√ºfung
- üü° **Medium**: Zivilrechtliche Folgen ‚Üí √úberarbeitung empfohlen
- üü¢ **Low**: Geringf√ºgige Verst√∂√üe ‚Üí Hinweis ausreichend

## üéØ Best Practices

### Schema-Auswahl f√ºr verschiedene Anwendungsf√§lle

#### üìö **Bildungsinhalte (Empfohlen)**
```json
{"schemes": ["overall_quality", "rechtliche_compliance"]}
```

#### üë• **User-Generated Content**
```json
{"schemes": ["jugendschutz_gate", "strafrecht_gate", "persoenlichkeitsrechte_gate"]}
```

#### üî¨ **Wissenschaftliche Texte**
```json
{"schemes": ["sachrichtigkeit_new", "neutralitaet_new"]}
```

#### ‚ö° **Schnelle Qualit√§tspr√ºfung**
```json
{"schemes": ["neutralitaet_old", "sachrichtigkeit_old"], "include_reasoning": false}
```

### Performance & Zuverl√§ssigkeit
- ‚úÖ **Automatische Fehlerbehandlung** f√ºr ung√ºltige Schema-IDs
- ‚úÖ **Timeout-Protection** f√ºr KI-Bewertungen
- ‚úÖ **Dependency-Validation** f√ºr derived Schemas
- ‚úÖ **Rate Limiting** ber√ºcksichtigen (OpenAI API)
- ‚úÖ **Retry-Logic** f√ºr tempor√§re Fehler

## üîß Entwicklung & Erweiterung

### API Status & Monitoring
```bash
# API Health Check
curl http://localhost:8001/health

# Alle verf√ºgbaren Schemas auflisten
curl http://localhost:8001/schemes

# Test-Bewertung durchf√ºhren
curl -X POST http://localhost:8001/evaluate \
  -H "Content-Type: application/json" \
  -d '{"text":"Testinhalt", "schemes":["neutralitaet_old"]}'
```

### Neues Schema erstellen
1. YAML-Datei in `schemes/` erstellen
2. Schema-Typ und Parameter definieren
3. Kriterien und Regeln spezifizieren
4. API neu starten f√ºr automatisches Laden

### Debugging & Logs
- ‚úÖ **Structured Logging** mit Loguru
- ‚úÖ **Request/Response Tracing**
- ‚úÖ **Error Details** in API Responses
- ‚úÖ **Schema Loading Status** beim Startup

## üìà Produktionsstatus

### ‚úÖ Funktionsf√§hige Features
- üü¢ **API Server**: L√§uft stabil auf Port 8001
- üü¢ **Alle Endpoints**: Health, Schemes, Evaluate funktionsf√§hig
- üü¢ **15 Bewertungsschemas**: Vollst√§ndig geladen und getestet
- üü¢ **KI-Integration**: OpenAI GPT-4 Bewertungen
- üü¢ **Swagger UI**: Vollst√§ndige interaktive Dokumentation
- üü¢ **Error Handling**: Robuste Fehlerbehandlung
- üü¢ **Schema Validation**: Pydantic-basierte Validierung

### üîÑ Letzte Updates (2025-01-18)
- ‚úÖ Fixed Swagger UI "Try it out" Funktionalit√§t
- ‚úÖ Erweiterte API-Dokumentation mit Beispielen
- ‚úÖ Verbesserte Schema-Validierung f√ºr Binary Gates
- ‚úÖ Dependency Injection Kompatibilit√§t

## üìù Lizenz & Rechtliches

Dieses System implementiert deutsche Rechtsstandards und ist f√ºr den Einsatz in Deutschland optimiert. Bei internationaler Nutzung sind lokale Gesetze zu beachten.

**‚ö†Ô∏è Haftungsausschluss**: Die automatische Rechtspr√ºfung ersetzt keine juristische Beratung. Bei kritischen Inhalten sollte immer eine manuelle Pr√ºfung erfolgen.

---

**üöÄ Ready for Production** | **ü§ñ LLM as Judge** | **üìö Full Documentation** | **üîß Easy to Extend**
